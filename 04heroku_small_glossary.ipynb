{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset of Glossary for HEROKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the larger glossary, pick only those SRC terms that are in tf-idf feature names. At the same\n",
    "# time, TRG translations must occur in the small corpus to be eligibile for inclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_DATA_PREFIX = '/Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   56460 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/lex_preproc40_cleaned\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {PROC_DATA_PREFIX}/lex_preproc40_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8419\n"
     ]
    }
   ],
   "source": [
    "# Build eng-rus dictionary\n",
    "en_ru_dict = dict()\n",
    "with open(PROC_DATA_PREFIX + '/' + 'lex_preproc40_cleaned', 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        src, trg = line.split('\\t')\n",
    "        try:\n",
    "            en_ru_dict[src.strip()].append(trg.strip())\n",
    "        except KeyError:\n",
    "             en_ru_dict[src.strip()] = [trg.strip()]\n",
    "\n",
    "print(len(en_ru_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in SRC and TRG:\n",
      "2452\n",
      "3638\n"
     ]
    }
   ],
   "source": [
    "# Collect vocabulary sets for small datasets\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "stopwords = ['the', 'a', 'an', 'of',\n",
    "#              's', 'and', 'и', 'or', 'или',\n",
    "#              'been', 'being', 'by'\n",
    "            ]\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Return a string cleaned up.\"\"\"\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # word-tokenize & remove numbers if the entire token consists of numbers\n",
    "    text = ' '.join(t for t in nltk.wordpunct_tokenize(text) if not t.isdigit() and not t in stopwords)\n",
    "    # remove punctuation\n",
    "    punct_remove = set(string.punctuation) | {'−', '\\t', '\\n', '\\r', '\\x0b', '\\x0c', '◦', '°'}\n",
    "    text = ''.join(char for char in text if char not in punct_remove)\n",
    "    # strip extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "voc_set_src = set()\n",
    "voc_set_trg = set()\n",
    "with open(PROC_DATA_PREFIX + '/' + 'en_ru_heroku_1000', 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        line_list = line.split('\\t')\n",
    "        src_str, trg_str = preprocess(line_list[0].strip()), preprocess(line_list[1].strip())\n",
    "        src, trg = src_str.split(), trg_str.split()\n",
    "        for tok in src:\n",
    "            voc_set_src.add(tok)\n",
    "        for tok in trg:\n",
    "            voc_set_trg.add(tok)\n",
    "print('Unique words in SRC and TRG:')            \n",
    "print(len(voc_set_src))\n",
    "print(len(voc_set_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     994 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/en_ru_heroku_1000\n",
      "     980 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/en_ru_heroku_tok_1000\n"
     ]
    }
   ],
   "source": [
    "# Nuber of lines in small raw and tok-ed files \n",
    "!wc -l {PROC_DATA_PREFIX}/'en_ru_heroku_1000'\n",
    "!wc -l {PROC_DATA_PREFIX}/'en_ru_heroku_tok_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    }
   ],
   "source": [
    "# Collect a vocabulary of most important words\n",
    "\n",
    "# Get a list of tok-ed sentences from the raw text file\n",
    "corpus_src = []\n",
    "with open(PROC_DATA_PREFIX + '/' + 'en_ru_heroku_1000', 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        src, _ = line.split('\\t')\n",
    "        corpus_src.append(preprocess(src).strip())\n",
    "print(len(corpus_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuber of features in SRC corpus:\n",
      " 2291\n",
      "First 50 features:\n",
      " ['7328th', 'aa', 'abdallah', 'abdelrahman', 'abdulkader', 'abdullah', 'abdulqader', 'absentia', 'abu', 'abubakar', 'abuses', 'abzar', 'academy', 'acceptable', 'acceptance', 'access', 'accommodations', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accounts', 'accuracy', 'accused', 'achievable', 'achieve', 'achieved', 'achievements', 'acknowledge', 'acordos', 'acoustic', 'act', 'acting', 'action', 'actions', 'active', 'actively', 'activities', 'activity', 'acts', 'adam', 'add', 'additional', 'address', 'addressing', 'administration', 'admiral', 'adolphus', 'adopt']\n"
     ]
    }
   ],
   "source": [
    "# Score vocab items (in iterations, starting from 1000 features and going up)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    max_features=2300,  # Keep increasing until the target of 9000 lines in Glossary is reached\n",
    ")\n",
    "tfidf_vectorizer.fit_transform(corpus_src)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print('Nuber of features in SRC corpus:\\n', len(feature_names))\n",
    "print('First 50 features:\\n', feature_names[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current number of important words:\n",
      " 2291\n",
      "resulting heroku glossary size in lines:\n",
      " 6827\n"
     ]
    }
   ],
   "source": [
    "# Based on the large glossary, pick only those SRC terms that are in feature names. At the same\n",
    "# time, TRG translations must occur in the small corpus to be eligibile for inclusion \n",
    "print('current number of important words:\\n', len(feature_names))\n",
    "heroku_glossary = []\n",
    "for src in feature_names:\n",
    "    try:\n",
    "        trg_list = en_ru_dict[src]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        for trg in trg_list:\n",
    "            if trg in voc_set_trg:\n",
    "                heroku_glossary.append('{}\\t{}\\n'.format(src, trg))\n",
    "\n",
    "print('resulting heroku glossary size in lines:\\n', len(heroku_glossary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Heroku glossary to file\n",
    "with open(PROC_DATA_PREFIX + '/' + 'heroku_glossary', 'w', encoding='utf8') as toF:\n",
    "    for line in heroku_glossary: \n",
    "        toF.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6827 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/heroku_glossary\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {PROC_DATA_PREFIX}/heroku_glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %history -g -f history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
