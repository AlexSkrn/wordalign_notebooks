{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract key phrases \n",
    " \n",
    " - as example queries for the web site\n",
    " - as an autocomplete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_DATA_PREFIX = '/Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = 'en_ru_heroku_1000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Return a string cleaned up.\"\"\"\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # word-tokenize & remove numbers if the entire token consists of numbers\n",
    "    text = ' '.join(t for t in nltk.wordpunct_tokenize(text) if not t.isdigit() and not t in stopwords.words('english'))\n",
    "    # remove punctuation\n",
    "    punct_remove = set(string.punctuation) | {'−', '\\t', '\\n', '\\r', '\\x0b', '\\x0c', '◦', '°'}\n",
    "    text = ''.join(char for char in text if char not in punct_remove)\n",
    "    # strip extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    }
   ],
   "source": [
    "corpus_src = []\n",
    "with open(PROC_DATA_PREFIX + '/' + corpus_file, 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        src, _ = line.split('\\t')\n",
    "        corpus_src.append(preprocess(src).strip())\n",
    "print(len(corpus_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('united nations', 39), ('secretary general', 39), ('security council', 18), ('paragraph resolution', 14), ('decides extend', 12), ('democratic republic', 11), ('republic congo', 11), ('member states', 11), ('international tribunal', 11), ('special tribunal', 11), ('humanitarian assistance', 10), ('requests secretary', 10), ('imposed paragraph', 9), ('measures imposed', 9), ('human rights', 9), ('general report', 9), ('decides remain', 9), ('seized matter', 9), ('specially designed', 9), ('extend mandate', 8), ('calls upon', 8), ('remain seized', 8), ('united states', 7), ('arms embargo', 7), ('armed groups', 7), ('troop contributing', 6), ('contributing countries', 6), ('displaced persons', 6), ('tribunal rwanda', 6), ('unmovic iaea', 6)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counter_2n = Counter()\n",
    "\n",
    "for text in corpus_src:\n",
    "    text = text.split()\n",
    "    word_counter_2n.update(ngrammer(text, n=2))\n",
    "\n",
    "print(word_counter_2n.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('democratic republic congo', 11), ('requests secretary general', 10), ('secretary general report', 9), ('decides extend mandate', 8), ('decides remain seized', 8), ('remain seized matter', 8), ('measures imposed paragraph', 7), ('troop contributing countries', 6), ('international tribunal rwanda', 6), ('charter united nations', 6), ('united states america', 5), ('imposed paragraph resolution', 5), ('responsibility identified committee', 5), ('report secretary general', 5), ('mandate united nations', 4), ('syrian arab republic', 4), ('disarmament demobilization reintegration', 4), ('president security council', 4), ('unmanned aerial vehicles', 4), ('condemns violations provisions', 4), ('violations provisions moscow', 4), ('provisions moscow agreement', 4), ('moscow agreement may', 4), ('agreement may ceasefire', 4), ('may ceasefire separation', 4), ('ceasefire separation forces', 4), ('separation forces annex', 4), ('lasting solution question', 4), ('solution question western', 4), ('question western sahara', 4)]\n"
     ]
    }
   ],
   "source": [
    "word_counter_3n = Counter()\n",
    "\n",
    "for text in corpus_src:\n",
    "    text = text.split()\n",
    "    word_counter_3n.update(ngrammer(text, n=3))\n",
    "\n",
    "print(word_counter_3n.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('decides remain seized matter', 8), ('condemns violations provisions moscow', 4), ('violations provisions moscow agreement', 4), ('provisions moscow agreement may', 4), ('moscow agreement may ceasefire', 4), ('agreement may ceasefire separation', 4), ('may ceasefire separation forces', 4), ('ceasefire separation forces annex', 4), ('lasting solution question western', 4), ('solution question western sahara', 4), ('technology development design production', 4), ('mindua democratic republic congo', 3), ('gon kwon republic korea', 3), ('measures imposed paragraph resolution', 3), ('requests secretary general report', 3), ('requests secretary general submit', 3), ('secretary general report council', 3), ('non civil certified aircraft', 3), ('unmanned aerial vehicles parts', 3), ('aerial vehicles parts components', 3), ('violations international humanitarian law', 3), ('development design production components', 3), ('design production components equipment', 3), ('antoine kesia mbe mindua', 2), ('kesia mbe mindua democratic', 2), ('mbe mindua democratic republic', 2), ('jean claude antonetti france', 2), ('theodor meron united states', 2), ('meron united states america', 2), ('support regional cooperation work', 2)]\n"
     ]
    }
   ],
   "source": [
    "word_counter_4n = Counter()\n",
    "\n",
    "for text in corpus_src:\n",
    "    text = text.split()\n",
    "    word_counter_4n.update(ngrammer(text, n=4))\n",
    "\n",
    "print(word_counter_4n.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('united', 'nations'),\n",
       " ('secretary', 'general'),\n",
       " ('seized', 'matter'),\n",
       " ('security', 'council'),\n",
       " ('decides', 'extend'),\n",
       " ('human', 'rights'),\n",
       " ('specially', 'designed'),\n",
       " ('republic', 'congo'),\n",
       " ('member', 'states'),\n",
       " ('remain', 'seized'),\n",
       " ('paragraph', 'resolution'),\n",
       " ('special', 'tribunal'),\n",
       " ('calls', 'upon'),\n",
       " ('democratic', 'republic'),\n",
       " ('measures', 'imposed'),\n",
       " ('owned', 'controlled'),\n",
       " ('arms', 'embargo'),\n",
       " ('humanitarian', 'assistance'),\n",
       " ('western', 'sahara'),\n",
       " ('imposed', 'paragraph'),\n",
       " ('ballistic', 'missile'),\n",
       " ('decides', 'remain'),\n",
       " ('troop', 'contributing'),\n",
       " ('armed', 'groups'),\n",
       " ('requests', 'secretary'),\n",
       " ('displaced', 'persons'),\n",
       " ('unmovic', 'iaea'),\n",
       " ('international', 'tribunal'),\n",
       " ('contributing', 'countries'),\n",
       " ('extend', 'mandate'),\n",
       " ('anti', 'tank'),\n",
       " ('côte', 'ivoire'),\n",
       " ('privileges', 'immunities'),\n",
       " ('russian', 'federation'),\n",
       " ('sierra', 'leone'),\n",
       " ('disarmament', 'demobilization'),\n",
       " ('lasting', 'solution'),\n",
       " ('unmanned', 'aerial'),\n",
       " ('separation', 'forces'),\n",
       " ('responsibility', 'identified'),\n",
       " ('trial', 'chambers'),\n",
       " ('long', 'term'),\n",
       " ('solution', 'question'),\n",
       " ('charter', 'united'),\n",
       " ('parts', 'components'),\n",
       " ('muammar', 'qadhafi'),\n",
       " ('tribunal', 'rwanda'),\n",
       " ('general', 'report'),\n",
       " ('states', 'america'),\n",
       " ('design', 'production')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coprus_list = []\n",
    "for text in corpus_src:\n",
    "    coprus_list.extend(text.split())\n",
    "    \n",
    "finder2 = BigramCollocationFinder.from_words(coprus_list)\n",
    "finder2.nbest(bigram_measures.likelihood_ratio, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['antoine', 'kesia', 'mbe', 'mindua', 'democratic', 'republic', 'congo'],\n",
       " ['howard', 'morrison', 'united', 'kingdom'],\n",
       " ['jean', 'claude', 'antonetti', 'france']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coprus_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an autocomplete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuber of features in SRC corpus:\n",
      " 500\n",
      "First 50 features:\n",
      " ['accordance', 'account', 'acting', 'action', 'activities', 'acts', 'additional', 'administration', 'adoption', 'aerial', 'afghanistan', 'african', 'agreement', 'aio', 'aircraft', 'aka', 'al', 'ambassador', 'america', 'ammunition', 'annex', 'anti', 'applicable', 'apply', 'appropriate', 'april', 'arab', 'areas', 'armed', 'arms', 'arrangements', 'article', 'assist', 'assistance', 'attached', 'august', 'authorities', 'authority', 'authorizes', 'ballistic', 'basis', 'behalf', 'calls', 'capacity', 'case', 'cease', 'ceasefire', 'central', 'certified', 'chambers']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    max_features=500,  # Keep increasing until the target of 9000 lines in Glossary is reached\n",
    ")\n",
    "tfidf_vectorizer.fit_transform(corpus_src)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print('Nuber of features in SRC corpus:\\n', len(feature_names))\n",
    "print('First 50 features:\\n', feature_names[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decides remain seized matter', 'condemns violations provisions moscow', 'violations provisions moscow agreement', 'provisions moscow agreement may', 'moscow agreement may ceasefire', 'agreement may ceasefire separation', 'may ceasefire separation forces', 'ceasefire separation forces annex', 'lasting solution question western', 'solution question western sahara']\n",
      "['victims', 'violations', 'violence', 'weapons', 'welcomes', 'welcoming', 'western', 'women', 'work', 'working']\n"
     ]
    }
   ],
   "source": [
    "# Build a list of 4-, 3-, 2-, and 1-grams \n",
    "autocomplete_list = []\n",
    "for ngram in word_counter_4n.most_common(30):\n",
    "    autocomplete_list.append(ngram[0])\n",
    "\n",
    "for ngram in word_counter_3n.most_common(50):\n",
    "    autocomplete_list.append(ngram[0])\n",
    "\n",
    "for ngram in word_counter_2n.most_common(100):\n",
    "    autocomplete_list.append(ngram[0])\n",
    "\n",
    "for ngram in feature_names:\n",
    "    autocomplete_list.append(ngram)\n",
    "\n",
    "print(autocomplete_list[:10])\n",
    "print(autocomplete_list[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "with open(PROC_DATA_PREFIX + '/' + 'auto_complete_eng', 'w', encoding='utf8') as toF:\n",
    "    toF.write('\\n'.join(autocomplete_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     680 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/auto_complete_eng\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {PROC_DATA_PREFIX}/auto_complete_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decides remain seized matter\n",
      "condemns violations provisions moscow\n",
      "work\n",
      "working\n"
     ]
    }
   ],
   "source": [
    "!head -2 {PROC_DATA_PREFIX}/auto_complete_eng\n",
    "!tail -2 {PROC_DATA_PREFIX}/auto_complete_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
