{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Russian->English search\n",
    "\n",
    "Similar to what I did in ```04heroku_small_glossary.ipynb``` and ```05key_phrases.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_DATA_PREFIX = '/Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   56460 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/lex_preproc40_cleaned\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {PROC_DATA_PREFIX}/lex_preproc40_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19614\n"
     ]
    }
   ],
   "source": [
    "# Build rus-eng dictionary\n",
    "ru_en_dict = dict()  # {'term': ['val1', 'val2', 'val3'], ...}\n",
    "with open(os.path.join(PROC_DATA_PREFIX, 'lex_preproc40_cleaned'), 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        trg, src = line.split('\\t')  # swap src and trg\n",
    "        try:\n",
    "            ru_en_dict[src.strip()].append(trg.strip())\n",
    "        except KeyError:\n",
    "             ru_en_dict[src.strip()] = [trg.strip()]\n",
    "\n",
    "print(len(ru_en_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nations', 'united']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_en_dict['объединенных']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in SRC and TRG:\n",
      "2452\n",
      "3638\n"
     ]
    }
   ],
   "source": [
    "# Collect vocabulary sets for small datasets\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "stopwords = ['the', 'a', 'an', 'of',\n",
    "#              'to',\n",
    "#              's', 'and', 'и', 'or', 'или',\n",
    "#              'been', 'being', 'by'\n",
    "            ]\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Return a string cleaned up.\"\"\"\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # word-tokenize & remove numbers if the entire token consists of numbers\n",
    "    text = ' '.join(t for t in nltk.wordpunct_tokenize(text) if not t.isdigit() and not t in stopwords)\n",
    "    # remove punctuation\n",
    "    punct_remove = set(string.punctuation) | {'−', '\\t', '\\n', '\\r', '\\x0b', '\\x0c', '◦', '°'}\n",
    "    text = ''.join(char for char in text if char not in punct_remove)\n",
    "    # strip extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "voc_set_src = set()\n",
    "voc_set_trg = set()\n",
    "with open(PROC_DATA_PREFIX + '/' + 'en_ru_heroku_1000', 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        line_list = line.split('\\t')\n",
    "        src_str, trg_str = preprocess(line_list[0].strip()), preprocess(line_list[1].strip())\n",
    "        src, trg = src_str.split(), trg_str.split()\n",
    "        for tok in src:\n",
    "            voc_set_src.add(tok)\n",
    "        for tok in trg:\n",
    "            voc_set_trg.add(tok)\n",
    "print('Unique words in SRC and TRG:')            \n",
    "print(len(voc_set_src))\n",
    "print(len(voc_set_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    }
   ],
   "source": [
    "# Collect the most important words\n",
    "\n",
    "# Get a list of tok-ed TRG sentences from the raw text file\n",
    "corpus_trg = []\n",
    "with open(PROC_DATA_PREFIX + '/' + 'en_ru_heroku_1000', 'r', encoding='utf8') as inF:\n",
    "    for line in inF:\n",
    "        _, trg = line.split('\\t')\n",
    "        corpus_trg.append(preprocess(trg).strip())\n",
    "print(len(corpus_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuber of features in TRG corpus:\n",
      " 2300\n",
      "First 50 features:\n",
      " ['55c', 'ii', 'iii', 'августа', 'авиационные', 'адвоката', 'администрации', 'адмирал', 'актах', 'активно', 'актов', 'акты', 'али', 'аль', 'америки', 'амисом', 'анализировать', 'английский', 'антонетти', 'антуан', 'апелляционной', 'аппараты', 'апреля', 'арабская', 'арабской', 'аспектов', 'аспекты', 'афганистана', 'афганистане', 'африканского', 'баллистическим', 'баллистических', 'банк', 'бахманьяр', 'беженцев', 'без', 'безопасного', 'безопасности', 'безопасность', 'беспилотные', 'бисау', 'ближайших', 'боевых', 'боеприпасы', 'более', 'борьбе', 'боснии', 'бригадный', 'будет', 'будут']\n"
     ]
    }
   ],
   "source": [
    "# Score vocab items (in iterations, starting from 1000 features and going up)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.7,  # automatically detect and filter stop words based on intra corpus document frequency of terms\n",
    "    sublinear_tf=True,\n",
    "    max_features=2300,  # Keep increasing until the target of 9000 lines in Glossary is reached\n",
    ")\n",
    "tfidf_vectorizer.fit_transform(corpus_trg)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print('Nuber of features in TRG corpus:\\n', len(feature_names))\n",
    "print('First 50 features:\\n', feature_names[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current number of important words:\n",
      " 2300\n",
      "resulting heroku glossary size in lines:\n",
      " 6430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ii\\tannex\\n',\n",
       " 'августа\\taugust\\n',\n",
       " 'авиационные\\taero\\n',\n",
       " 'авиационные\\tair\\n',\n",
       " 'авиационные\\tcapabilities\\n']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on the large glossary, pick only those TRG terms that are in feature names. At the same\n",
    "# time, SRC terms must occur in the small corpus to be eligibile for inclusion \n",
    "print('current number of important words:\\n', len(feature_names))\n",
    "heroku_glossary = []\n",
    "for trg in feature_names:\n",
    "    try:\n",
    "        src_list = ru_en_dict[trg]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        for src in src_list:\n",
    "            if src in voc_set_src:\n",
    "                heroku_glossary.append('{}\\t{}\\n'.format(trg, src))\n",
    "\n",
    "print('resulting heroku glossary size in lines:\\n', len(heroku_glossary))\n",
    "heroku_glossary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6430 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/heroku_glossary_trg\r\n"
     ]
    }
   ],
   "source": [
    "# Write Heroku glossary to file\n",
    "with open(PROC_DATA_PREFIX + '/' + 'heroku_glossary_trg', 'w', encoding='utf8') as toF:\n",
    "    for line in heroku_glossary: \n",
    "        toF.write(line)\n",
    "!wc -l {PROC_DATA_PREFIX}/heroku_glossary_trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Phrases for AutoComplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "def corpus_ngram_counter(corpus, n):\n",
    "    \"\"\"Given a list of tok-ed sentences and n for # of grams, return a counter.\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for text in corpus:\n",
    "        text = text.split()\n",
    "        word_counter.update(ngrammer(text, n))\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter_2n = corpus_ngram_counter(corpus_trg, 2)\n",
    "word_counter_3n = corpus_ngram_counter(corpus_trg, 3)\n",
    "word_counter_4n = corpus_ngram_counter(corpus_trg, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('объединенных наций', 39), ('организации объединенных', 37), ('генерального секретаря', 30), ('в отношении', 15), ('в пункте', 14), ('постановляет продлить', 13), ('мая года', 13), ('совета безопасности', 13), ('в соответствии', 12), ('г н', 12), ('соответствии с', 11), ('гуманитарной помощи', 10), ('о своей', 10), ('специального трибунала', 9), ('декабря года', 9), ('постановляет что', 9), ('просит генерального', 9), ('постановляет продолжать', 9), ('в целях', 8), ('настоящей резолюции', 8), ('связи с', 8), ('безопасности и', 8), ('прав человека', 8), ('от мая', 8), ('года s', 8), ('заниматься этим', 8), ('этим вопросом', 8), ('июня года', 8), ('международного трибунала', 8), ('наций в', 7)] \n",
      "\n",
      "[('организации объединенных наций', 36), ('в соответствии с', 11), ('просит генерального секретаря', 9), ('от мая года', 8), ('заниматься этим вопросом', 8), ('объединенных наций в', 7), ('соглашения о прекращении', 7), ('в связи с', 7), ('оказанию гуманитарной помощи', 7), ('постановляет продолжать заниматься', 7), ('продолжать заниматься этим', 7), ('постановляет продлить мандат', 6), ('в пункте резолюции', 6), ('о прекращении огня', 6), ('содействие оказанию гуманитарной', 6), ('доклад генерального секретаря', 6), ('мая года s', 6), ('в собственности или', 6), ('собственности или под', 6), ('или под контролем', 6), ('демократическая республика конго', 5), ('по поддержанию мира', 5), ('эмбарго на поставки', 5), ('на поставки оружия', 5), ('прекращении огня и', 5), ('огня и разъединении', 5), ('и разъединении сил', 5), ('мер предусмотренных в', 5), ('выполняет любые другие', 5), ('любые другие функции', 5)] \n",
      "\n",
      "[('организации объединенных наций', 36), ('в соответствии с', 11), ('просит генерального секретаря', 9), ('от мая года', 8), ('заниматься этим вопросом', 8), ('объединенных наций в', 7), ('соглашения о прекращении', 7), ('в связи с', 7), ('оказанию гуманитарной помощи', 7), ('постановляет продолжать заниматься', 7), ('продолжать заниматься этим', 7), ('постановляет продлить мандат', 6), ('в пункте резолюции', 6), ('о прекращении огня', 6), ('содействие оказанию гуманитарной', 6), ('доклад генерального секретаря', 6), ('мая года s', 6), ('в собственности или', 6), ('собственности или под', 6), ('или под контролем', 6), ('демократическая республика конго', 5), ('по поддержанию мира', 5), ('эмбарго на поставки', 5), ('на поставки оружия', 5), ('прекращении огня и', 5), ('огня и разъединении', 5), ('и разъединении сил', 5), ('мер предусмотренных в', 5), ('выполняет любые другие', 5), ('любые другие функции', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(word_counter_2n.most_common(30), '\\n')\n",
    "print(word_counter_3n.most_common(30), '\\n')\n",
    "print(word_counter_3n.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuber of features in TRG corpus:\n",
      " 500\n",
      "First 50 features:\n",
      " ['ii', 'августа', 'администрации', 'аль', 'америки', 'аппараты', 'апреля', 'арабская', 'африканского', 'беженцев', 'безопасности', 'безопасность', 'более', 'будет', 'будут', 'будучи', 'было', 'быть', 'важность', 'ваоонвс', 'введенные', 'вклад', 'включая', 'власти', 'властям', 'внимание', 'вновь', 'во', 'военные', 'военных', 'войска', 'вооруженных', 'вопроса', 'вопросам', 'вопросом', 'восточной', 'время', 'все', 'всеми', 'всех', 'выборов', 'выполняет', 'выполнять', 'выражая', 'высоко', 'выше', 'гаити', 'генерал', 'генерального', 'генеральным']\n"
     ]
    }
   ],
   "source": [
    "# Collect 500 unigrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.7,  # automatically detect and filter stop words based on intra corpus document frequency of terms\n",
    "    sublinear_tf=True,\n",
    "    max_features=500,\n",
    ")\n",
    "tfidf_vectorizer.fit_transform(corpus_trg)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print('Number of features in TRG corpus:\\n', len(feature_names))\n",
    "print('First 50 features:\\n', feature_names[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['организации объединенных наций в', 'постановляет продолжать заниматься этим', 'продолжать заниматься этим вопросом', 'соглашения о прекращении огня', 'содействие оказанию гуманитарной помощи', 'в собственности или под', 'собственности или под контролем', 'эмбарго на поставки оружия', 'о прекращении огня и', 'прекращении огня и разъединении']\n",
      "['этих', 'это', 'этого', 'этой', 'этому', 'юнмовик', 'юрисдикция', 'является', 'являются', 'января']\n"
     ]
    }
   ],
   "source": [
    "# Build a list of 4-, 3-, 2-, and 1-grams \n",
    "autocomplete_list = []\n",
    "for ngram in word_counter_4n.most_common(30):\n",
    "    autocomplete_list.append(ngram[0])\n",
    "\n",
    "for ngram in word_counter_3n.most_common(50):\n",
    "    autocomplete_list.append(ngram[0])\n",
    "\n",
    "for ngram in word_counter_2n.most_common(100):\n",
    "    autocomplete_list.append(ngram[0])\n",
    "\n",
    "for ngram in feature_names:\n",
    "    autocomplete_list.append(ngram)\n",
    "\n",
    "print(autocomplete_list[:10])\n",
    "print(autocomplete_list[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     680 /Users/alexskrn/Documents/NLP/WordAlign/wordalign_notebooks/data/auto_complete_rus\n",
      "организации объединенных наций в\n",
      "постановляет продолжать заниматься этим\n",
      "являются\n",
      "января\n"
     ]
    }
   ],
   "source": [
    "# Write to file\n",
    "with open(PROC_DATA_PREFIX + '/' + 'auto_complete_rus', 'w', encoding='utf8') as toF:\n",
    "    toF.write('\\n'.join(autocomplete_list) + '\\n')\n",
    "    \n",
    "    \n",
    "!wc -l {PROC_DATA_PREFIX}/auto_complete_rus\n",
    "!head -2 {PROC_DATA_PREFIX}/auto_complete_rus\n",
    "!tail -2 {PROC_DATA_PREFIX}/auto_complete_rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
